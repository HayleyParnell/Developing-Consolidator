{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f3d9b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f922c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check date format and return True if valid, False otherwise\n",
    "def validate_and_convert_date(date_str):\n",
    "    if date_str.strip():  # Check if not empty or null\n",
    "        try:\n",
    "            # Use '%m/%d/%y' format for validation and conversion\n",
    "            return datetime.strptime(date_str, '%m/%d/%y')\n",
    "        except ValueError:\n",
    "            return None  # Return None for invalid date strings\n",
    "    return None  # Return None for empty strings or non-string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74b9e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_unique(x):\n",
    "    return ', '.join(set(filter(None, map(str.strip, x))))\n",
    "\n",
    "def join_unique_address(x):\n",
    "    return '/ '.join(set(filter(None, map(str.strip, x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5504e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "input_folder_path = 'C:/Files for SHB4 - Copy/Resources/'\n",
    "final_output_file_path = 'C:/Files for SHB4 - Copy/final_df1.csv'\n",
    "levels_json_file = 'C:/Files for SHB4 - Copy/levels_Revised.json'\n",
    "files = os.listdir(input_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a028797",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    'Associated Docs (People)': str,\n",
    "    'People Tracker Control Number': str,\n",
    "    'First Name or Initial': str,\n",
    "    'Middle Name or Initial': str,\n",
    "    'Last Name': str,\n",
    "    'City': str,\n",
    "    'State': str,\n",
    "    'ZIP Code': str,\n",
    "    'Date of Birth': str,\n",
    "    'Financial Account Number': str\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72d8811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store DataFrames\n",
    "dfs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34396ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each file and read it into a DataFrame, then append to the list\n",
    "for file in files:\n",
    "    if file.endswith('.csv'):  # Check if the file is a CSV file\n",
    "        file_path = os.path.join(input_folder_path, file)  # Get the full file path\n",
    "        df = pd.read_csv(file_path, dtype=dtype_dict,encoding='latin1')\n",
    "        dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0577255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine DataFrames\n",
    "combined_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd43d3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Row Duplicates\n",
    "\n",
    "combined_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cee83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with empty string ('') for non-numeric columns\n",
    "non_numeric_columns = combined_df.select_dtypes(exclude=['number']).columns\n",
    "combined_df[non_numeric_columns] = combined_df[non_numeric_columns].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "218ed182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN and empty strings in 'Social Security Number' column\n",
    "combined_df = combined_df[\n",
    "    (combined_df['Social Security Number'].notnull()) &  # Not NaN\n",
    "    (combined_df['Social Security Number'] != '')       # Not empty string\n",
    "].sort_values(by='Social Security Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa751117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you want to convert the 'Date of Birth' back to string format with the desired format\n",
    "# combined_df['Date of Birth'] = combined_df['Date of Birth'].dt.strftime('%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6409d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the ZIP Codes, State, City fields\n",
    "valid_zip_mask = combined_df['ZIP Code'].str.match(r'^\\d{5}$') | (combined_df['ZIP Code'] == '')\n",
    "valid_state_mask = combined_df['State'].str.match(r'^[A-Z]{2}$') | (combined_df['State'] == '')\n",
    "valid_city_mask = combined_df['City'].str.match(r'^[a-zA-Z\\s]+$') | (combined_df['City'] == '')\n",
    "filtered_df = combined_df[valid_zip_mask & valid_state_mask & valid_city_mask]\n",
    "filtered_df['Address'] = filtered_df['Address'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65f100b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing Nan's with empty strings\n",
    "df_sorted = filtered_df.applymap(lambda x: '' if pd.isna(x) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f36c60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "with open(levels_json_file, 'r') as file:\n",
    "    levels_data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e8c3a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove quotations around 'join_unique' in aggregation functions\n",
    "for level_data in levels_data.values():\n",
    "    aggregation_functions = level_data['aggregation_functions']\n",
    "    for col, func in aggregation_functions.items():\n",
    "        if func == 'join_unique':\n",
    "            aggregation_functions[col] = join_unique\n",
    "        if func == 'join_unique_address':\n",
    "            aggregation_functions[col] = join_unique_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f718495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped.drop(columns=['Date of Birth'], inplace=True)  # Drop the existing 'Date of Birth' column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d27b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the levels and perform aggregation\n",
    "for level_name, level_data in levels_data.items():\n",
    "    columns = level_data['columns']\n",
    "    aggregation_functions = level_data['aggregation_functions']\n",
    "    df_grouped = filtered_df.groupby(columns).agg(aggregation_functions).reset_index()\n",
    "    # Perform further operations with filtered_df as needed\n",
    "    print(f\"Grouping by {level_name} with columns - {columns} completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba690a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72421cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the final output to a file\n",
    "df_grouped.to_csv(final_output_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
