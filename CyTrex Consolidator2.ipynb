{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32f4c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f90ffef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "input_folder_path = 'C:/SHB0004/Resources/'\n",
    "final_output_file_path = 'C:/SHB0004/Consolidated_Entity List2.xlsx'\n",
    "levels_json_file = 'C:/SHB0004/Levels_Revised.json'\n",
    "files = os.listdir(input_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57fe8b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check date format and return True if valid, False otherwise\n",
    "def validate_and_convert_date(date_str):\n",
    "    if date_str.strip():  # Check if not empty or null\n",
    "        try:\n",
    "            # Use '%m/%d/%y' format for validation and conversion\n",
    "            return datetime.strptime(date_str, '%m/%d/%y')\n",
    "        except ValueError:\n",
    "            return None  # Return None for invalid date strings\n",
    "    return None  # Return None for empty strings or non-string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "901a5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_unique(x):\n",
    "    return ', '.join(set(filter(None, map(str.strip, x))))\n",
    "\n",
    "def join_unique_address(x):\n",
    "    return '/ '.join(set(filter(None, map(str.strip, x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f62cf599",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    'Associated Docs (People)': str,\n",
    "    'People Tracker Control Number': str,\n",
    "    'First Name or Initial': str,\n",
    "    'Middle Name or Initial': str,\n",
    "    'Last Name': str,\n",
    "    'City': str,\n",
    "    'State': str,\n",
    "    'ZIP Code': str,\n",
    "    'Date of Birth': str,\n",
    "    'Financial Account Number': str\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fea6790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store DataFrames\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c35901a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each file and read it into a DataFrame, then append to the list\n",
    "for file in files:\n",
    "    if file.endswith('.xlsx') or file.endswith('.xlsx'):  # Check if the file is an Excel file\n",
    "        file_path = os.path.join(input_folder_path, file)  # Get the full file path\n",
    "        df = pd.read_excel(file_path, dtype=dtype_dict)\n",
    "        dfs.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b642ecad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine DataFrames\n",
    "combined_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16af7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Row Duplicates\n",
    "combined_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b03f20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace NaN values with blanks\n",
    "combined_df.fillna('', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46588977",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "011e0c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter rows based on valid date format in 'Date of Birth' column\n",
    "# combined_df['Date of Birth'] = combined_df['Date of Birth'].apply(validate_and_convert_date)\n",
    "# # Assuming df_sorted is your DataFrame\n",
    "# combined_df['Date of Birth'] = pd.to_datetime(combined_df['Date of Birth'], errors='coerce')\n",
    "# combined_df['Date of Birth'] = combined_df['Date of Birth'].fillna(pd.to_datetime('1900-01-01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4f79132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # If you want to convert the 'Date of Birth' back to string format with the desired format\n",
    "# combined_df['Date of Birth'] = combined_df['Date of Birth'].dt.strftime('%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "580d7b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.fillna({'Suffix': '', 'State': '', 'ZIP Code': '', 'Country (if not USA)': ''}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e515489",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b7df25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the ZIP Codes, State, City fields\n",
    "valid_zip_mask = combined_df['ZIP Code'].str.match(r'^\\d{5}$') | (combined_df['ZIP Code'] == '')\n",
    "valid_state_mask = combined_df['State'].str.match(r'^[A-Z]{2}$') | (combined_df['State'] == '')\n",
    "valid_city_mask = combined_df['City'].str.match(r'^[a-zA-Z\\s]+$') | (combined_df['City'] == '')\n",
    "filtered_df = combined_df[valid_zip_mask & valid_state_mask & valid_city_mask]\n",
    "filtered_df.loc[:, 'Address'] = filtered_df['Address'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8199d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "838c86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "with open(levels_json_file, 'r') as file:\n",
    "    levels_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efa8206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove quotations around 'join_unique' in aggregation functions\n",
    "for level_data in levels_data.values():\n",
    "    aggregation_functions = level_data['aggregation_functions']\n",
    "    for col, func in aggregation_functions.items():\n",
    "        if func == 'join_unique':\n",
    "            aggregation_functions[col] = join_unique\n",
    "        if func == 'join_unique_address':\n",
    "            aggregation_functions[col] = join_unique_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e91406ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping by level1 with columns - ['Social Security Number', 'First Name or Initial', 'Last Name', 'Suffix', 'Date of Birth'] completed.\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the levels and perform aggregation\n",
    "for level_name, level_data in levels_data.items():\n",
    "    columns = level_data['columns']\n",
    "    aggregation_functions = level_data['aggregation_functions']\n",
    "    df_grouped = filtered_df.groupby(columns).agg(aggregation_functions).reset_index()\n",
    "    # Perform further operations with df_grouped as needed\n",
    "    print(f\"Grouping by {level_name} with columns - {columns} completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0324f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06a50959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the 'firstname', 'lastname', 'middle name', and 'dob' columns by commas and expand them into separate columns\n",
    "# df_split = pd.concat([df_grouped[col].str.split(',', expand=True).add_prefix(f'{col}_') for col in ['First Name or Initial', 'Last Name', 'Middle Name or Initial', 'Date of Birth']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5c46298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate the split columns with the original DataFrame\n",
    "# df_combined = pd.concat([df_grouped, df_split], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0cd5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to an Excel file\n",
    "df_grouped.to_excel(final_output_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
